{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://medium.com/@kbdhunga/advanced-rag-multi-query-retriever-approach-ad8cd0ea0f5b\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "#         )\n",
    "#    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index and store in vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IVR\\github\\ai_venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Spliting \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decomposition prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. \"What is the Multi Query method in the context of RAG technique?\"',\n",
       " '2. \"How does the Multi Query method differ from the basic RAG technique?\"',\n",
       " '3. \"What are the benefits of using the Multi Query method to enhance the RAG technique?\"',\n",
       " '4. \"What are some examples of how the Multi Query method has been used to improve the RAG technique?\"']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(temperature=0,api_key=os.getenv('groq_api_key'))\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run - Main Question\n",
    "question = \"How does the Multi Query method enhance the basic RAG technique?\"\n",
    "\n",
    "generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IVR\\github\\ai_venv\\Lib\\site-packages\\langsmith\\client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "C:\\Users\\IVR\\AppData\\Local\\Temp\\ipykernel_20656\\39558728.py:6: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(sub_question)\n"
     ]
    }
   ],
   "source": [
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "sub_questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "rag_results = []\n",
    "for sub_question in sub_questions:\n",
    "  retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "  answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs,\n",
    "                                                                \"question\": sub_question})\n",
    "  rag_results.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: 1. \"What is the Multi Query method in the context of RAG technique?\"\n",
      "Answer 1: The Multi Query method in the context of the RAG (Retrieval-Augmented Generation) technique refers to a process where multiple sets of documents are retrieved based on varied interpretations of the original query. This approach is particularly beneficial for vague queries, as it harnesses the power of diversity in the retrieved documents to produce the final output.\n",
      "\n",
      "Question 2: 2. \"How does the Multi Query method differ from the basic RAG technique?\"\n",
      "Answer 2: The Multi Query method in RAG (Retrieval-Augmented Generation) differs from the basic RAG technique in that it retrieves multiple sets of documents based on varied interpretations of the original query, as opposed to the basic method which retrieves a singular set of documents for an initial query. This makes the Multi Query method more suitable for vague or imprecisely formulated queries.\n",
      "\n",
      "Question 3: 3. \"What are the benefits of using the Multi Query method to enhance the RAG technique?\"\n",
      "Answer 3: The Multi Query method in the RAG technique is beneficial for handling vague or imprecise queries by retrieving multiple sets of documents based on varied interpretations of the original query, increasing the likelihood of finding the most relevant and accurate answers. This approach reduces the dependence on a singular set of documents and mitigates the impact of query phrasing variations on the final outcome.\n",
      "\n",
      "Question 4: 4. \"What are some examples of how the Multi Query method has been used to improve the RAG technique?\"\n",
      "Answer 4: The Multi Query method in RAG (Retrieval-Augmented Generation) improves the technique by addressing vague or imprecise queries through casting a wider net with multiple queries. This increases the likelihood of pinpointing the most relevant and accurate answers from a vast amount of documents. Instead of relying on a singular set of retrieved documents for the initial query, it harnesses the power of diversity by retrieving multiple sets of documents based on varied interpretations of the original query.\n"
     ]
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(sub_questions, rag_results)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Multi Query method enhances the basic RAG (Retrieval-Augmented Generation) technique by introducing a process that retrieves multiple sets of documents based on varied interpretations of the original query. This approach differs from the basic RAG technique, which retrieves a singular set of documents for an initial query. The Multi Query method is particularly beneficial for handling vague or imprecise queries, as it increases the likelihood of finding the most relevant and accurate answers by reducing the dependence on a singular set of documents and mitigating the impact of query phrasing variations on the final outcome. This method has been used to improve the RAG technique by addressing vague or imprecise queries through casting a wider net with multiple queries, thereby increasing the likelihood of pinpointing the most relevant and accurate answers from a vast amount of documents.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
